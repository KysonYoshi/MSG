{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339789",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = 'YOUR_API_KEY'\n",
    "model_name = 'gpt-4o-mini' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ee72fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box_matcher No param\n",
      "obj_embedder frozen\n",
      "place_embedder frozen\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ast\n",
    "import astunparse\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "import heapq\n",
    "import base64\n",
    "\n",
    "\n",
    "import localization \n",
    "from localization import build_msg_localizer\n",
    "\n",
    "video_id = \"41069042\"\n",
    "predicted_msg_file = \"./QA_agent/mini-val/\" + video_id + \"/refine_topo_gt.json\"\n",
    "localizer = build_msg_localizer(\n",
    "    msg_path = predicted_msg_file,\n",
    "    video_id = video_id,\n",
    "    experiment_mode=\"localize\",\n",
    "    device = 0,\n",
    "    split = \"mini-val\",\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38bf8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMP:\n",
    "\n",
    "    def __init__(self, name, cfg, lmp_fgen, fixed_vars, variable_vars):\n",
    "        self._name = name\n",
    "        self._cfg = cfg\n",
    "\n",
    "        self._base_prompt = self._cfg['prompt_text']\n",
    "\n",
    "        self._stop_tokens = list(self._cfg['stop'])\n",
    "\n",
    "        self._lmp_fgen = lmp_fgen\n",
    "\n",
    "        self._fixed_vars = fixed_vars\n",
    "        self._variable_vars = variable_vars\n",
    "        self.exec_hist = ''\n",
    "\n",
    "    def clear_exec_hist(self):\n",
    "        self.exec_hist = ''\n",
    "\n",
    "    def build_prompt(self, query, context=''):\n",
    "        if len(self._variable_vars) > 0:\n",
    "            variable_vars_imports_str = f\"from utils import {', '.join(self._variable_vars.keys())}\"\n",
    "        else:\n",
    "            variable_vars_imports_str = ''\n",
    "        prompt = self._base_prompt.replace('{variable_vars_imports}', variable_vars_imports_str)\n",
    "\n",
    "        if self._cfg['maintain_session']:\n",
    "            prompt += f'\\n{self.exec_hist}'\n",
    "\n",
    "        if context != '':\n",
    "            prompt += f'\\n{context}'\n",
    "\n",
    "        use_query = f'{self._cfg[\"query_prefix\"]}{query}{self._cfg[\"query_suffix\"]}'\n",
    "        prompt += f'\\n{use_query}'\n",
    "\n",
    "        return prompt, use_query\n",
    "\n",
    "    def __call__(self, query, context='', **kwargs):\n",
    "        prompt, use_query = self.build_prompt(query, context=context)\n",
    "        messages = [{\"role\": \"system\", \"content\": \"user are doing few-shot prompting. Please provide the Python code without enclosing it in triple backticks.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                code_str = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    stop=self._stop_tokens,\n",
    "                    temperature=self._cfg['temperature'],\n",
    "                    model=self._cfg['engine'],\n",
    "                    max_tokens=self._cfg['max_tokens']\n",
    "                )\n",
    "                code_str = code_str.choices[0].message.content\n",
    "                break\n",
    "            except (RateLimitError, APIConnectionError) as e:\n",
    "                print(f'OpenAI API got err {e}')\n",
    "                print('Retrying after 10s.')\n",
    "                sleep(10)\n",
    "        if self._cfg['include_context'] and context != '':\n",
    "            to_exec = f'{context}\\n{code_str}'\n",
    "            to_log = f'{context}\\n{use_query}\\n{code_str}'\n",
    "        else:\n",
    "            to_exec = code_str\n",
    "            to_log = f'{use_query}\\n{to_exec}'\n",
    "\n",
    "        to_log_pretty = highlight(to_log, PythonLexer(), TerminalFormatter())\n",
    "        print(f'LMP {self._name} exec:\\n\\n{to_log_pretty}\\n')\n",
    "\n",
    "        new_fs = self._lmp_fgen.create_new_fs_from_code(code_str)\n",
    "        self._variable_vars.update(new_fs)\n",
    "\n",
    "        gvars = merge_dicts([self._fixed_vars, self._variable_vars])\n",
    "        lvars = kwargs\n",
    "\n",
    "        if not self._cfg['debug_mode']:\n",
    "            exec_safe(to_exec, gvars, lvars)\n",
    "\n",
    "        self.exec_hist += f'\\n{to_exec}'\n",
    "\n",
    "        if self._cfg['maintain_session']:\n",
    "            self._variable_vars.update(lvars)\n",
    "\n",
    "        if self._cfg['has_return']:\n",
    "            return lvars[self._cfg['return_val_name']]\n",
    "\n",
    "\n",
    "class LMPFGen:\n",
    "\n",
    "    def __init__(self, cfg, fixed_vars, variable_vars):\n",
    "        self._cfg = cfg\n",
    "\n",
    "        self._stop_tokens = list(self._cfg['stop'])\n",
    "        self._fixed_vars = fixed_vars\n",
    "        self._variable_vars = variable_vars\n",
    "\n",
    "        self._base_prompt = self._cfg['prompt_text']\n",
    "\n",
    "    def create_f_from_sig(self, f_name, f_sig, other_vars=None, fix_bugs=False, return_src=False):\n",
    "        print(f'Creating function: {f_sig}')\n",
    "\n",
    "        use_query = f'{self._cfg[\"query_prefix\"]}{f_sig}{self._cfg[\"query_suffix\"]}'\n",
    "        prompt = f'{self._base_prompt}\\n{use_query}'\n",
    "        messages = [{\"role\": \"system\", \"content\": \"user are doing few-shot prompting. Please provide the Python code without enclosing it in triple backticks.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                f_src = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    stop=self._stop_tokens,\n",
    "                    temperature=self._cfg['temperature'],\n",
    "                    model=self._cfg['engine'],\n",
    "                    max_tokens=self._cfg['max_tokens']\n",
    "                )\n",
    "                f_src = f_src.choices[0].message.content\n",
    "                break\n",
    "            except (RateLimitError, APIConnectionError) as e:\n",
    "                print(f'OpenAI API got err {e}')\n",
    "                print('Retrying after 10s.')\n",
    "                sleep(10)\n",
    "\n",
    "        if fix_bugs:\n",
    "            f_src = openai.Edit.create(\n",
    "                model='gpt-4o-mini',\n",
    "                input='# ' + f_src,\n",
    "                temperature=0,\n",
    "                instruction='Fix the bug if there is one. Improve readability. Keep same inputs and outputs. Only small changes. No comments.',\n",
    "            )['choices'][0]['text'].strip()\n",
    "\n",
    "        if other_vars is None:\n",
    "            other_vars = {}\n",
    "        gvars = merge_dicts([self._fixed_vars, self._variable_vars, other_vars])\n",
    "        lvars = {}\n",
    "        \n",
    "        exec_safe(f_src, gvars, lvars)\n",
    "\n",
    "        f = lvars[f_name]\n",
    "\n",
    "        to_print = highlight(f'{use_query}\\n{f_src}', PythonLexer(), TerminalFormatter())\n",
    "        print(f'LMP FGEN created:\\n\\n{to_print}\\n')\n",
    "\n",
    "        if return_src:\n",
    "            return f, f_src\n",
    "        return f\n",
    "\n",
    "    def create_new_fs_from_code(self, code_str, other_vars=None, fix_bugs=False, return_src=False):\n",
    "        fs, f_assigns = {}, {}\n",
    "        f_parser = FunctionParser(fs, f_assigns)\n",
    "        f_parser.visit(ast.parse(code_str))\n",
    "        for f_name, f_assign in f_assigns.items():\n",
    "            if f_name in fs:\n",
    "                fs[f_name] = f_assign\n",
    "\n",
    "        if other_vars is None:\n",
    "            other_vars = {}\n",
    "\n",
    "        new_fs = {}\n",
    "        srcs = {}\n",
    "        for f_name, f_sig in fs.items():\n",
    "            all_vars = merge_dicts([self._fixed_vars, self._variable_vars, new_fs, other_vars])\n",
    "            if not var_exists(f_name, all_vars):\n",
    "                f, f_src = self.create_f_from_sig(f_name, f_sig, new_fs, fix_bugs=fix_bugs, return_src=True)\n",
    "\n",
    "                # recursively define child_fs in the function body if needed\n",
    "                f_def_body = astunparse.unparse(ast.parse(f_src).body[0].body)\n",
    "                child_fs, child_f_srcs = self.create_new_fs_from_code(\n",
    "                    f_def_body, other_vars=all_vars, fix_bugs=fix_bugs, return_src=True\n",
    "                )\n",
    "\n",
    "                if len(child_fs) > 0:\n",
    "                    new_fs.update(child_fs)\n",
    "                    srcs.update(child_f_srcs)\n",
    "\n",
    "                    # redefine parent f so newly created child_fs are in scope\n",
    "                    gvars = merge_dicts([self._fixed_vars, self._variable_vars, new_fs, other_vars])\n",
    "                    lvars = {}\n",
    "                    \n",
    "                    exec_safe(f_src, gvars, lvars)\n",
    "                    \n",
    "                    f = lvars[f_name]\n",
    "\n",
    "                new_fs[f_name], srcs[f_name] = f, f_src\n",
    "\n",
    "        if return_src:\n",
    "            return new_fs, srcs\n",
    "        return new_fs\n",
    "\n",
    "\n",
    "class FunctionParser(ast.NodeTransformer):\n",
    "\n",
    "    def __init__(self, fs, f_assigns):\n",
    "        super().__init__()\n",
    "        self._fs = fs\n",
    "        self._f_assigns = f_assigns\n",
    "\n",
    "    def visit_Call(self, node):\n",
    "        self.generic_visit(node)\n",
    "        if isinstance(node.func, ast.Name):\n",
    "            f_sig = astunparse.unparse(node).strip()\n",
    "            f_name = astunparse.unparse(node.func).strip()\n",
    "            self._fs[f_name] = f_sig\n",
    "        return node\n",
    "\n",
    "    def visit_Assign(self, node):\n",
    "        self.generic_visit(node)\n",
    "        if isinstance(node.value, ast.Call):\n",
    "            assign_str = astunparse.unparse(node).strip()\n",
    "            f_name = astunparse.unparse(node.value.func).strip()\n",
    "            self._f_assigns[f_name] = assign_str\n",
    "        return node\n",
    "\n",
    "\n",
    "def var_exists(name, all_vars):\n",
    "    try:\n",
    "        eval(name, all_vars)\n",
    "    except:\n",
    "        exists = False\n",
    "    else:\n",
    "        exists = True\n",
    "    return exists\n",
    "\n",
    "\n",
    "def merge_dicts(dicts):\n",
    "    return {\n",
    "        k : v \n",
    "        for d in dicts\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "    \n",
    "\n",
    "def exec_safe(code_str, gvars=None, lvars=None):\n",
    "    #banned_phrases = ['import', '__']\n",
    "    banned_phrases = []\n",
    "    for phrase in banned_phrases:\n",
    "        assert phrase not in code_str\n",
    "  \n",
    "    if gvars is None:\n",
    "        gvars = {}\n",
    "    if lvars is None:\n",
    "        lvars = {}\n",
    "    empty_fn = lambda *args, **kwargs: None\n",
    "    custom_gvars = merge_dicts([\n",
    "        gvars,\n",
    "        {'exec': empty_fn, 'eval': empty_fn}\n",
    "    ])\n",
    "    exec(code_str, custom_gvars, lvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dfa9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class MultiviewSceneGraph():\n",
    "    def __init__(self):\n",
    "        #init\n",
    "        with open(predicted_msg_file, 'r', encoding='utf-8') as file:\n",
    "            self.json_data = json.load(file)\n",
    "            \n",
    "    def map_uids_to_keys(self, obj_list):\n",
    "        reverse_map = {}\n",
    "        for key, uids in self.json_data['uidmap'].items():\n",
    "            for uid in uids:\n",
    "                reverse_map[uid] = key\n",
    "\n",
    "        mapped_result = []\n",
    "        for uid in obj_list:\n",
    "            if uid in reverse_map:\n",
    "                key = reverse_map[uid]\n",
    "                mapped_result.append(key)\n",
    "\n",
    "        return mapped_result\n",
    "    \n",
    "    def get_number_of_frames(self):\n",
    "        return len(self.json_data[\"sampled_frames\"])\n",
    "    \n",
    "    def get_frame2index(self, frame_number):\n",
    "        return self.json_data['sampled_frames'].index(frame_number)\n",
    "\n",
    "    def get_img2frame(self, img_name):\n",
    "        img_path = './QA_agent/input_img/' + img_name + '.png'\n",
    "        loc, _ = localizer.localize(img_path)\n",
    "        return loc\n",
    "    \n",
    "    def get_index2frame(self, index):\n",
    "        return self.json_data['sampled_frames'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1317a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LMP_wrapper():\n",
    "    def __init__(self, env, cfg, render=False):\n",
    "        self.env = env\n",
    "        self._cfg = cfg\n",
    "        \n",
    "    def get_frame2index(self, frame_number):\n",
    "        return self.env.get_frame2index(frame_number)\n",
    "\n",
    "    def get_img2frame(self, img_name):\n",
    "        return self.env.get_img2frame(img_name)\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "    \n",
    "    def get_index2frame(self, index):\n",
    "        return self.env.get_index2frame(index)\n",
    "        \n",
    "    def get_number_of_frames(self):\n",
    "        return self.env.get_number_of_frames()\n",
    "    \n",
    "    def show_images_from_frames(self, frame_list):\n",
    "        num_images = len(frame_list)\n",
    "        num_cols = 3  \n",
    "        num_rows = (num_images + num_cols - 1) // num_cols  \n",
    "\n",
    "        plt.figure(figsize=(15, num_rows * 5))\n",
    "        image_paths = [\"./QA_agent/mini-val/\" + video_id + \"/\" + video_id + '_frames/lowres_wide/' + video_id + '_' + frame_number + '.png' for frame_number in frame_list]\n",
    "\n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            if os.path.exists(image_path):\n",
    "                image = Image.open(image_path)\n",
    "                plt.subplot(num_rows, num_cols, i + 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(os.path.basename(image_path))\n",
    "                plt.axis('off')  \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def count_objects(self, objects):\n",
    "        object_count = {}\n",
    "        for obj in objects:\n",
    "            if obj in object_count:\n",
    "                object_count[obj] += 1\n",
    "            else:\n",
    "                object_count[obj] = 1\n",
    "        return object_count\n",
    "    \n",
    "    def get_object_from_frame(self, frame_name):\n",
    "        object_item_list = {}\n",
    "        for a_key, a_values in self.env.json_data['annotations'][frame_name].items():\n",
    "            for u_key, u_values in self.env.json_data['uidmap'].items():\n",
    "                if a_key in u_values:\n",
    "                    object_item_list[a_key] = u_key\n",
    "        return object_item_list\n",
    "    \n",
    "    def get_object(self, object_name):\n",
    "        object_list = {}\n",
    "        if object_name in self.env.json_data[\"uidmap\"]:\n",
    "            for item in self.env.json_data[\"uidmap\"][object_name]:\n",
    "                object_list[item] = object_name\n",
    "        else:\n",
    "            obj = self.issimilar(object_name, self.env.json_data[\"uidmap\"])\n",
    "            if obj != '':\n",
    "                object_list = self.get_object(obj)\n",
    "        return object_list\n",
    "    \n",
    "    def get_object_frames(self, object_list):\n",
    "        place_list = []\n",
    "        for key_to_find in object_list:\n",
    "            time_stamps = [time_stamp for time_stamp, keys in self.env.json_data['annotations'].items() if key_to_find in keys]\n",
    "            place_list = place_list + time_stamps\n",
    "\n",
    "        return place_list\n",
    "    \n",
    "    def shortest_path(self, start, goal):\n",
    "        graph = self.env.json_data['p-p']\n",
    "        n = len(graph)\n",
    "        distances = {node: float('inf') for node in range(n)}\n",
    "        distances[start] = 0\n",
    "        priority_queue = [(0, start)]\n",
    "        previous_nodes = {node: None for node in range(n)}\n",
    "\n",
    "        while priority_queue:\n",
    "            current_distance, current_node = heapq.heappop(priority_queue)\n",
    "\n",
    "            if current_node == goal:\n",
    "                path = []\n",
    "                while previous_nodes[current_node] is not None:\n",
    "                    path.append(current_node)\n",
    "                    current_node = previous_nodes[current_node]\n",
    "                path.append(start)\n",
    "                return path[::-1]\n",
    "\n",
    "            if current_distance > distances[current_node]:\n",
    "                continue\n",
    "\n",
    "            for neighbor, weight in enumerate(graph[current_node]):\n",
    "                if weight > 0:\n",
    "                    distance = current_distance + weight\n",
    "                    if distance < distances[neighbor]:\n",
    "                        distances[neighbor] = distance\n",
    "                        previous_nodes[neighbor] = current_node\n",
    "                        heapq.heappush(priority_queue, (distance, neighbor))\n",
    "\n",
    "        return None  \n",
    "    \n",
    "    def get_object_uids_list(self, frame_name):\n",
    "        object_item_list = {}\n",
    "        for a_key, a_values in self.env.json_data['annotations'][frame_name].items():\n",
    "            for u_key, u_values in self.env.json_data['uidmap'].items():\n",
    "                if a_key in u_values:\n",
    "                    object_item_list[a_key] = u_key\n",
    "        return object_item_list\n",
    "    \n",
    "    def issimilar(self, obj, object_list):\n",
    "        new_prompt = f'is {obj} in {object_list}'\n",
    "        messages = [{\"role\": \"system\", \"content\": \"user are asking if the given object things are in the object_list. Please just return the object name in object_list. if not, return \"\". For example: object = 'tv', object_list = {'bed': ['NB59gmIiC4u5h2Mw'], 'table': ['RnVg7UM3yU93OL1o', '53naDCpgHHmCVkxd'], 'cabinet': ['BOYx4gvUEXXzkHo0', 'FbEfcoVRieMmQ4IW'], 'tv_monitor': ['qJ0TKTnoAkhV0k0C']} This should return 'tv_monitor'. object = 'book', object_list = {'bed': ['NB59gmIiC4u5h2Mw'], 'table': ['RnVg7UM3yU93OL1o', '53naDCpgHHmCVkxd'], 'cabinet': ['BOYx4gvUEXXzkHo0', 'FbEfcoVRieMmQ4IW'], 'tv_monitor': ['qJ0TKTnoAkhV0k0C']} This should return ''  \"},\n",
    "                    {\"role\": \"user\", \"content\": new_prompt}]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def vlm(self, frame_number, text):\n",
    "        image_path = \"./QA_agent/mini-val/\" + video_id + \"/\" + video_id + '_frames/lowres_wide/' + video_id + '_' + frame_number + '.png'\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            img = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "          model=\"gpt-4o-mini\",\n",
    "          messages=[\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\"type\": \"text\", \"text\": text},\n",
    "                {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{img}\",\n",
    "                  },\n",
    "                },\n",
    "              ],\n",
    "            }\n",
    "          ],\n",
    "          max_tokens=300,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e7ffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tabletop_ui = '''\n",
    "#python EQA agent script\n",
    "#Generate the corresponding code according to the corresponding question\n",
    "\n",
    "#question: Show how many sampled_frames in this sample.\n",
    "#code:\n",
    "num = get_number_of_frames()\n",
    "say('f'There are {num} frames')\n",
    "\n",
    "#question: Show what kind of objects are there in frame number 3044.239 and the objects' quantities.\n",
    "#code:\n",
    "objects = get_object_from_frame(\"3044.239\")\n",
    "object_quantities = count_objects(objects)\n",
    "say(f'The objects in frame 3064.730 are: {object_quantities}')\n",
    "\n",
    "#question: Where is the table?\n",
    "#code:\n",
    "objects = get_object(\"table\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi I find there is a table in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: I am tired where can I go to sleep.\n",
    "#Hint: find bed!\n",
    "#code:\n",
    "objects = get_object(\"bed\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi I find there is a bed in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: Where can I put my bottle?\n",
    "#Hint: find table!\n",
    "#code:\n",
    "objects = get_object(\"table\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi you can put your bottle on the table: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: I am hungry where can I go to eat.\n",
    "#Hint: find table!\n",
    "#code:\n",
    "objects = get_object(\"table\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi I find there is a table in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "\n",
    "#question: \"My current position is in picture 'start', how can I go to picture 'goal'?\"\n",
    "#code:\n",
    "start_frame = get_img2frame(\"start\")\n",
    "goal_frame = get_img2frame(\"goal\")\n",
    "start_node = get_frame2index(start_frame)\n",
    "goal_node = get_frame2index(goal_frame)\n",
    "path = shortest_path(start_node, goal_node)\n",
    "frame_path = [get_index2frame(i) for i in path]\n",
    "say(f'Your path is {frame_path}')\n",
    "show_images_from_frames(frame_path)\n",
    "\n",
    "#question: \"are the tables in frame 3044.239 and frame 3105.730 the same?\"\n",
    "#code:\n",
    "objects_frame_1 = get_object_from_frame(\"3044.239\")\n",
    "objects_frame_2 = get_object_from_frame(\"3105.730\")\n",
    "tables_frame_1 = [key for key, values in objects_frame_1.items() if issimilar(values,[\"table\"])]\n",
    "tables_frame_2 = [key for key, values in objects_frame_2.items() if issimilar(values,[\"table\"])]\n",
    "\n",
    "if isequal(tables_frame_1,  tables_frame_2):\n",
    "    say(\"The tables in frame 3044.239 and frame 3105.730 are the same.\")\n",
    "else:\n",
    "    say(\"The tables in frame 3044.239 and frame 3105.730 are not the same.\")\n",
    "show_images_from_frames([\"3044.239\", \"3105.730\"])\n",
    "\n",
    "\n",
    "#question: does the table in frame number: 3044.239 appear in other frames?\n",
    "#code:\n",
    "objects = get_object_from_frame(\"3044.239\")\n",
    "target_objects = {key: values for key, values in objects.items() if issimilar(values,[\"table\"])}\n",
    "frames = get_object_frames(target_objects)\n",
    "say(f'Hi I find there is the same table in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#important!!! if you get any question regarding color, shape, texture... anything you think scene-graph representations can't solve, than use the following vlm() funciton.\n",
    "#question: What's the color of the cabinet in picture 3123.722?\n",
    "#code:\n",
    "response = vlm(\"3123.722\", \"What's the color of the cabinet?\")\n",
    "say(f'{response}')\n",
    "show_images_from_frames([\"3123.722\"])\n",
    "\n",
    "#question: \n",
    "#code:\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe69bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fgen = '''\n",
    "#Generate the corresponding code according to the corresponding function define.\n",
    "\n",
    "#define function: ind = get_index(index)\n",
    "#code:\n",
    "def get_index(index):\n",
    "    return get_index2frame(index)\n",
    "    \n",
    "#define function: object = get_all_objects_in_space()\n",
    "#code:\n",
    "def get_all_objects_in_space():\n",
    "    object_list = []\n",
    "    env = get_env()\n",
    "    for item in env.json_data[\"uidmap\"].keys():\n",
    "        object_list.append(item)\n",
    "    return object_list\n",
    "    \n",
    "# define function: object_uids_list = get_object_from_frame(frame_name).\n",
    "#code:\n",
    "def get_object_from_frame(frame_name):\n",
    "    object_item_list = {}\n",
    "    env = get_env()\n",
    "    for a_key, a_values in env.json_data['annotations'][frame_name].items():\n",
    "        for u_key, u_values in env.json_data['uidmap'].items():\n",
    "            if a_key in u_values:\n",
    "                object_item_list[a_key] = u_key\n",
    "    return object_item_list\n",
    "\n",
    "#define function: result = isequal(a, b)\n",
    "#code:\n",
    "def isequal(a, b):\n",
    "    if a == b:\n",
    "        return True\n",
    "    return False\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f09218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_tabletop = {\n",
    "  'lmps': {\n",
    "    'tabletop_ui': {\n",
    "      'prompt_text': prompt_tabletop_ui,\n",
    "      'engine': model_name,\n",
    "      'max_tokens': 512,\n",
    "      'temperature': 0,\n",
    "      'query_prefix': '# ',\n",
    "      'query_suffix': '.',\n",
    "      'stop': ['#', 'objects = ['],\n",
    "      'maintain_session': True,\n",
    "      'debug_mode': False,\n",
    "      'include_context': True,\n",
    "      'has_return': False,\n",
    "      'return_val_name': 'ret_val',\n",
    "    },\n",
    "    'fgen': {\n",
    "      'prompt_text': prompt_fgen,\n",
    "      'engine': model_name,\n",
    "      'max_tokens': 512,\n",
    "      'temperature': 0,\n",
    "      'query_prefix': '# define function: ',\n",
    "      'query_suffix': '.',\n",
    "      'stop': ['# define', '# example'],\n",
    "      'maintain_session': False,\n",
    "      'debug_mode': False,\n",
    "      'include_context': True,\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aac3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_LMP(env, cfg_tabletop):\n",
    "    # LMP env wrapper\n",
    "    cfg_tabletop = copy.deepcopy(cfg_tabletop)\n",
    "    LMP_env = LMP_wrapper(env, cfg_tabletop)  \n",
    "    \n",
    "    # creating APIs that the LMPs can interact with\n",
    "    fixed_vars = {\n",
    "        'np': np, 'heapq': heapq\n",
    "    }\n",
    "    variable_vars = {\n",
    "      k: getattr(LMP_env, k)\n",
    "      for k in [\n",
    "         'get_env', 'get_index2frame', 'get_frame2index', 'get_img2frame', 'show_images_from_frames',\n",
    "          'get_number_of_frames', 'count_objects', 'get_object_from_frame', 'get_object',\n",
    "          'get_object_frames', 'shortest_path', 'vlm', 'issimilar'\n",
    "      ]\n",
    "    }\n",
    "    variable_vars['say'] = lambda msg: print(f'robot says: {msg}')\n",
    "\n",
    "    # creating the function-generating LMP\n",
    "    lmp_fgen = LMPFGen(cfg_tabletop['lmps']['fgen'], fixed_vars, variable_vars)\n",
    "    \n",
    "    # creating other low-level LMPs\n",
    "    variable_vars.update({\n",
    "      k: LMP(k, cfg_tabletop['lmps'][k], lmp_fgen, fixed_vars, variable_vars)\n",
    "      for k in []\n",
    "    })\n",
    "    # creating the LMP that deals w/ high-level language commands\n",
    "    lmp_tabletop_ui = LMP(\n",
    "      'tabletop_ui', cfg_tabletop['lmps']['tabletop_ui'], lmp_fgen, fixed_vars, variable_vars\n",
    "    )\n",
    "\n",
    "    return lmp_tabletop_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c1bc61d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MultiviewSceneGraph()\n",
    "lmp_tabletop_ui = setup_LMP(env, cfg_tabletop)\n",
    "\n",
    "#Example questions for the model to process\n",
    "#question = \"How many frames are there in the .\"\n",
    "#question = \"what kind of objects are there in the frame number 3044.722 and also give me the quantities\"\n",
    "#question = \"show me where are the tables.\"\n",
    "#question = \"Where can I put my laptop.\"\n",
    "#question = \"I am tired where can I go to sleep\"\n",
    "question = \"I’m in place 'start', how can I go to 'goal'?\"\n",
    "##question = \"what do we have in this space\"\n",
    "\n",
    "#question = \"are the tables in frame 3044.239 and frame 3105.730 the same?\"\n",
    "#question = \"are the tables in frame 3044.239 and frame 3044.722 the same?\"\n",
    "#question = \"does the table in frame number: 3044.239 appear in other frames?\"\n",
    "#question = \"does the tv in frame number: 3127.721 appear in other frames?\"\n",
    "#question = \"is there any book in this space?\"\n",
    "#question = \"How many tv are there.\"\n",
    "#question = \"What's the color of the cabinet in picture 3123.722?\"\n",
    "#question = \"What's the shape of the table in picture 3044.239?\"\n",
    "\n",
    "user_input = question #@param {allow-input: true, type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d1facc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RateLimitError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 45\u001b[0m, in \u001b[0;36mLMP.__call__\u001b[0;34m(self, query, context, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     code_str \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     46\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     47\u001b[0m         stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_tokens,\n\u001b[1;32m     48\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     49\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     code_str \u001b[38;5;241m=\u001b[39m code_str\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/msg/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/msg/lib/python3.11/site-packages/openai/resources/chat/completions.py:742\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    741\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    744\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    745\u001b[0m         {\n\u001b[1;32m    746\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    747\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    750\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    751\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    752\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    755\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    756\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    757\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    758\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    759\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    760\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    761\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    762\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    763\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    767\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    768\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    769\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    772\u001b[0m         },\n\u001b[1;32m    773\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    774\u001b[0m     ),\n\u001b[1;32m    775\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    776\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    777\u001b[0m     ),\n\u001b[1;32m    778\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    779\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    780\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    781\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/msg/lib/python3.11/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1274\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m )\n\u001b[0;32m-> 1277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/msg/lib/python3.11/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    955\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    956\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    957\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    958\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    959\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    960\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/msg/lib/python3.11/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************m8um. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lmp_tabletop_ui(user_input, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 54\u001b[0m, in \u001b[0;36mLMP.__call__\u001b[0;34m(self, query, context, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     code_str \u001b[38;5;241m=\u001b[39m code_str\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RateLimitError, APIConnectionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI API got err \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrying after 10s.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RateLimitError' is not defined"
     ]
    }
   ],
   "source": [
    "lmp_tabletop_ui(user_input, f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57415ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fee84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "1. \"Added object comparison for specific video frames\"\n",
    "2. \"Added command to check whether object appearance in specific frames\"\n",
    "3. \"Added `issimilar` function for fuzzy object name matching\"\n",
    "4. \"Added VLM access.\"\n",
    "5. \"Added localization function.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
