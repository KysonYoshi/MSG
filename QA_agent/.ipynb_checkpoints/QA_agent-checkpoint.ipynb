{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339789",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = 'YOUR_API_KEY'\n",
    "model_name = 'gpt-4o-mini' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee72fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astunparse in /home/cl6933/miniconda3/envs/msg/lib/python3.11/site-packages (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/cl6933/miniconda3/envs/msg/lib/python3.11/site-packages (from astunparse) (0.44.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/cl6933/miniconda3/envs/msg/lib/python3.11/site-packages (from astunparse) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cl6933/miniconda3/envs/msg/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'configs/experiments/localize.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#predicted_msg_file = \"/home/jz4725/msg/exp-results/hpc-damsg-mse/2024-05-14_22-26-52/Test/41069042/eval_results.json\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m predicted_msg_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./QA_agent/mini-val/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m video_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/refine_topo_gt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m localizer \u001b[38;5;241m=\u001b[39m build_msg_localizer(\n\u001b[1;32m     26\u001b[0m     msg_path \u001b[38;5;241m=\u001b[39m predicted_msg_file,\n\u001b[1;32m     27\u001b[0m     video_id \u001b[38;5;241m=\u001b[39m video_id,\n\u001b[1;32m     28\u001b[0m     experiment_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     30\u001b[0m     split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini-val\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai_api_key\n\u001b[1;32m     34\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI()\n",
      "File \u001b[0;32m~/MSG/localization.py:151\u001b[0m, in \u001b[0;36mbuild_msg_localizer\u001b[0;34m(msg_path, video_id, model_path, experiment_mode, device, split)\u001b[0m\n\u001b[1;32m    145\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n\u001b[1;32m    146\u001b[0m args\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: experiment_mode,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_split\u001b[39m\u001b[38;5;124m\"\u001b[39m: split,\n\u001b[1;32m    150\u001b[0m })\n\u001b[0;32m--> 151\u001b[0m config \u001b[38;5;241m=\u001b[39m get_configs(base_config_dir, args, creat_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# get model\u001b[39;00m\n\u001b[1;32m    154\u001b[0m device_no \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/MSG/util/config_utils.py:51\u001b[0m, in \u001b[0;36mget_configs\u001b[0;34m(base_config_dir, args, creat_subdir)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mexperiment:\n\u001b[1;32m     50\u001b[0m     experiment_config_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/experiments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     experiment_config \u001b[38;5;241m=\u001b[39m load_yaml(experiment_config_path)\n\u001b[1;32m     52\u001b[0m     final_config \u001b[38;5;241m=\u001b[39m merge_configs(base_config, experiment_config)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# print(\"simply use base\")\u001b[39;00m\n",
      "File \u001b[0;32m~/MSG/util/config_utils.py:10\u001b[0m, in \u001b[0;36mload_yaml\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_yaml\u001b[39m(file_path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'configs/experiments/localize.yaml'"
     ]
    }
   ],
   "source": [
    "!pip install astunparse\n",
    "\n",
    "import openai\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import ast\n",
    "import astunparse\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "import heapq\n",
    "import base64\n",
    "\n",
    "sys.path.append(os.path.abspath('..')) \n",
    "\n",
    "import localization \n",
    "from localization import build_msg_localizer\n",
    "\n",
    "video_id = \"41069042\"\n",
    "#predicted_msg_file = \"/home/jz4725/msg/exp-results/hpc-damsg-mse/2024-05-14_22-26-52/Test/41069042/eval_results.json\"\n",
    "predicted_msg_file = \"./QA_agent/mini-val/\" + video_id + \"/refine_topo_gt.json\"\n",
    "localizer = build_msg_localizer(\n",
    "    msg_path = predicted_msg_file,\n",
    "    video_id = video_id,\n",
    "    experiment_mode=\"localize\",\n",
    "    device = 0,\n",
    "    split = \"mini-val\",\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMP:\n",
    "\n",
    "    def __init__(self, name, cfg, lmp_fgen, fixed_vars, variable_vars):\n",
    "        self._name = name\n",
    "        self._cfg = cfg\n",
    "\n",
    "        self._base_prompt = self._cfg['prompt_text']\n",
    "\n",
    "        self._stop_tokens = list(self._cfg['stop'])\n",
    "\n",
    "        self._lmp_fgen = lmp_fgen\n",
    "\n",
    "        self._fixed_vars = fixed_vars\n",
    "        self._variable_vars = variable_vars\n",
    "        self.exec_hist = ''\n",
    "\n",
    "    def clear_exec_hist(self):\n",
    "        self.exec_hist = ''\n",
    "\n",
    "    def build_prompt(self, query, context=''):\n",
    "        if len(self._variable_vars) > 0:\n",
    "            variable_vars_imports_str = f\"from utils import {', '.join(self._variable_vars.keys())}\"\n",
    "        else:\n",
    "            variable_vars_imports_str = ''\n",
    "        prompt = self._base_prompt.replace('{variable_vars_imports}', variable_vars_imports_str)\n",
    "\n",
    "        if self._cfg['maintain_session']:\n",
    "            prompt += f'\\n{self.exec_hist}'\n",
    "\n",
    "        if context != '':\n",
    "            prompt += f'\\n{context}'\n",
    "\n",
    "        use_query = f'{self._cfg[\"query_prefix\"]}{query}{self._cfg[\"query_suffix\"]}'\n",
    "        prompt += f'\\n{use_query}'\n",
    "\n",
    "        return prompt, use_query\n",
    "\n",
    "    def __call__(self, query, context='', **kwargs):\n",
    "        prompt, use_query = self.build_prompt(query, context=context)\n",
    "        messages = [{\"role\": \"system\", \"content\": \"user are doing few-shot prompting. Please provide the Python code without enclosing it in triple backticks.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                code_str = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    stop=self._stop_tokens,\n",
    "                    temperature=self._cfg['temperature'],\n",
    "                    model=self._cfg['engine'],\n",
    "                    max_tokens=self._cfg['max_tokens']\n",
    "                )\n",
    "                code_str = code_str.choices[0].message.content\n",
    "                break\n",
    "            except (RateLimitError, APIConnectionError) as e:\n",
    "                print(f'OpenAI API got err {e}')\n",
    "                print('Retrying after 10s.')\n",
    "                sleep(10)\n",
    "        if self._cfg['include_context'] and context != '':\n",
    "            to_exec = f'{context}\\n{code_str}'\n",
    "            to_log = f'{context}\\n{use_query}\\n{code_str}'\n",
    "        else:\n",
    "            to_exec = code_str\n",
    "            to_log = f'{use_query}\\n{to_exec}'\n",
    "\n",
    "        to_log_pretty = highlight(to_log, PythonLexer(), TerminalFormatter())\n",
    "        print(f'LMP {self._name} exec:\\n\\n{to_log_pretty}\\n')\n",
    "\n",
    "        new_fs = self._lmp_fgen.create_new_fs_from_code(code_str)\n",
    "        self._variable_vars.update(new_fs)\n",
    "\n",
    "        gvars = merge_dicts([self._fixed_vars, self._variable_vars])\n",
    "        lvars = kwargs\n",
    "\n",
    "        if not self._cfg['debug_mode']:\n",
    "            exec_safe(to_exec, gvars, lvars)\n",
    "\n",
    "        self.exec_hist += f'\\n{to_exec}'\n",
    "\n",
    "        if self._cfg['maintain_session']:\n",
    "            self._variable_vars.update(lvars)\n",
    "\n",
    "        if self._cfg['has_return']:\n",
    "            return lvars[self._cfg['return_val_name']]\n",
    "\n",
    "\n",
    "class LMPFGen:\n",
    "\n",
    "    def __init__(self, cfg, fixed_vars, variable_vars):\n",
    "        self._cfg = cfg\n",
    "\n",
    "        self._stop_tokens = list(self._cfg['stop'])\n",
    "        self._fixed_vars = fixed_vars\n",
    "        self._variable_vars = variable_vars\n",
    "\n",
    "        self._base_prompt = self._cfg['prompt_text']\n",
    "\n",
    "    def create_f_from_sig(self, f_name, f_sig, other_vars=None, fix_bugs=False, return_src=False):\n",
    "        print(f'Creating function: {f_sig}')\n",
    "\n",
    "        use_query = f'{self._cfg[\"query_prefix\"]}{f_sig}{self._cfg[\"query_suffix\"]}'\n",
    "        prompt = f'{self._base_prompt}\\n{use_query}'\n",
    "        messages = [{\"role\": \"system\", \"content\": \"user are doing few-shot prompting. Please provide the Python code without enclosing it in triple backticks.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                f_src = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    stop=self._stop_tokens,\n",
    "                    temperature=self._cfg['temperature'],\n",
    "                    model=self._cfg['engine'],\n",
    "                    max_tokens=self._cfg['max_tokens']\n",
    "                )\n",
    "                f_src = f_src.choices[0].message.content\n",
    "                break\n",
    "            except (RateLimitError, APIConnectionError) as e:\n",
    "                print(f'OpenAI API got err {e}')\n",
    "                print('Retrying after 10s.')\n",
    "                sleep(10)\n",
    "\n",
    "        if fix_bugs:\n",
    "            f_src = openai.Edit.create(\n",
    "                model='gpt-4o-mini',\n",
    "                input='# ' + f_src,\n",
    "                temperature=0,\n",
    "                instruction='Fix the bug if there is one. Improve readability. Keep same inputs and outputs. Only small changes. No comments.',\n",
    "            )['choices'][0]['text'].strip()\n",
    "\n",
    "        if other_vars is None:\n",
    "            other_vars = {}\n",
    "        gvars = merge_dicts([self._fixed_vars, self._variable_vars, other_vars])\n",
    "        lvars = {}\n",
    "        \n",
    "        exec_safe(f_src, gvars, lvars)\n",
    "\n",
    "        f = lvars[f_name]\n",
    "\n",
    "        to_print = highlight(f'{use_query}\\n{f_src}', PythonLexer(), TerminalFormatter())\n",
    "        print(f'LMP FGEN created:\\n\\n{to_print}\\n')\n",
    "\n",
    "        if return_src:\n",
    "            return f, f_src\n",
    "        return f\n",
    "\n",
    "    def create_new_fs_from_code(self, code_str, other_vars=None, fix_bugs=False, return_src=False):\n",
    "        fs, f_assigns = {}, {}\n",
    "        f_parser = FunctionParser(fs, f_assigns)\n",
    "        f_parser.visit(ast.parse(code_str))\n",
    "        for f_name, f_assign in f_assigns.items():\n",
    "            if f_name in fs:\n",
    "                fs[f_name] = f_assign\n",
    "\n",
    "        if other_vars is None:\n",
    "            other_vars = {}\n",
    "\n",
    "        new_fs = {}\n",
    "        srcs = {}\n",
    "        for f_name, f_sig in fs.items():\n",
    "            all_vars = merge_dicts([self._fixed_vars, self._variable_vars, new_fs, other_vars])\n",
    "            if not var_exists(f_name, all_vars):\n",
    "                f, f_src = self.create_f_from_sig(f_name, f_sig, new_fs, fix_bugs=fix_bugs, return_src=True)\n",
    "\n",
    "                # recursively define child_fs in the function body if needed\n",
    "                f_def_body = astunparse.unparse(ast.parse(f_src).body[0].body)\n",
    "                child_fs, child_f_srcs = self.create_new_fs_from_code(\n",
    "                    f_def_body, other_vars=all_vars, fix_bugs=fix_bugs, return_src=True\n",
    "                )\n",
    "\n",
    "                if len(child_fs) > 0:\n",
    "                    new_fs.update(child_fs)\n",
    "                    srcs.update(child_f_srcs)\n",
    "\n",
    "                    # redefine parent f so newly created child_fs are in scope\n",
    "                    gvars = merge_dicts([self._fixed_vars, self._variable_vars, new_fs, other_vars])\n",
    "                    lvars = {}\n",
    "                    \n",
    "                    exec_safe(f_src, gvars, lvars)\n",
    "                    \n",
    "                    f = lvars[f_name]\n",
    "\n",
    "                new_fs[f_name], srcs[f_name] = f, f_src\n",
    "\n",
    "        if return_src:\n",
    "            return new_fs, srcs\n",
    "        return new_fs\n",
    "\n",
    "\n",
    "class FunctionParser(ast.NodeTransformer):\n",
    "\n",
    "    def __init__(self, fs, f_assigns):\n",
    "        super().__init__()\n",
    "        self._fs = fs\n",
    "        self._f_assigns = f_assigns\n",
    "\n",
    "    def visit_Call(self, node):\n",
    "        self.generic_visit(node)\n",
    "        if isinstance(node.func, ast.Name):\n",
    "            f_sig = astunparse.unparse(node).strip()\n",
    "            f_name = astunparse.unparse(node.func).strip()\n",
    "            self._fs[f_name] = f_sig\n",
    "        return node\n",
    "\n",
    "    def visit_Assign(self, node):\n",
    "        self.generic_visit(node)\n",
    "        if isinstance(node.value, ast.Call):\n",
    "            assign_str = astunparse.unparse(node).strip()\n",
    "            f_name = astunparse.unparse(node.value.func).strip()\n",
    "            self._f_assigns[f_name] = assign_str\n",
    "        return node\n",
    "\n",
    "\n",
    "def var_exists(name, all_vars):\n",
    "    try:\n",
    "        eval(name, all_vars)\n",
    "    except:\n",
    "        exists = False\n",
    "    else:\n",
    "        exists = True\n",
    "    return exists\n",
    "\n",
    "\n",
    "def merge_dicts(dicts):\n",
    "    return {\n",
    "        k : v \n",
    "        for d in dicts\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "    \n",
    "\n",
    "def exec_safe(code_str, gvars=None, lvars=None):\n",
    "    #banned_phrases = ['import', '__']\n",
    "    banned_phrases = []\n",
    "    for phrase in banned_phrases:\n",
    "        assert phrase not in code_str\n",
    "  \n",
    "    if gvars is None:\n",
    "        gvars = {}\n",
    "    if lvars is None:\n",
    "        lvars = {}\n",
    "    empty_fn = lambda *args, **kwargs: None\n",
    "    custom_gvars = merge_dicts([\n",
    "        gvars,\n",
    "        {'exec': empty_fn, 'eval': empty_fn}\n",
    "    ])\n",
    "    exec(code_str, custom_gvars, lvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfa9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class MultiviewSceneGraph():\n",
    "    def __init__(self):\n",
    "        #init\n",
    "        with open(predicted_msg_file, 'r', encoding='utf-8') as file:\n",
    "            self.json_data = json.load(file)\n",
    "            \n",
    "    def map_uids_to_keys(self, obj_list):\n",
    "        reverse_map = {}\n",
    "        for key, uids in self.json_data['uidmap'].items():\n",
    "            for uid in uids:\n",
    "                reverse_map[uid] = key\n",
    "\n",
    "        mapped_result = []\n",
    "        for uid in obj_list:\n",
    "            if uid in reverse_map:\n",
    "                key = reverse_map[uid]\n",
    "                mapped_result.append(key)\n",
    "\n",
    "        return mapped_result\n",
    "    \n",
    "    def get_number_of_frames(self):\n",
    "        return len(self.json_data[\"sampled_frames\"])\n",
    "    \n",
    "    def get_frame2index(self, frame_number):\n",
    "        return self.json_data['sampled_frames'].index(frame_number)\n",
    "    \n",
    "    def get_index2frame(self, index):\n",
    "        return self.json_data['sampled_frames'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1317a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LMP_wrapper():\n",
    "    def __init__(self, env, cfg, render=False):\n",
    "        self.env = env\n",
    "        self._cfg = cfg\n",
    "        \n",
    "    def get_frame2index(self, frame_number):\n",
    "        return self.env.get_frame2index(frame_number)\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "    \n",
    "    def get_index2frame(self, index):\n",
    "        return self.env.get_index2frame(index)\n",
    "        \n",
    "    def get_number_of_frames(self):\n",
    "        return self.env.get_number_of_frames()\n",
    "    \n",
    "    def show_images_from_frames(self, frame_list):\n",
    "        num_images = len(frame_list)\n",
    "        num_cols = 3  \n",
    "        num_rows = (num_images + num_cols - 1) // num_cols  \n",
    "\n",
    "        plt.figure(figsize=(15, num_rows * 5))\n",
    "        image_paths = ['41069042_frames/lowres_wide/41069042_' + frame_number + '.png' for frame_number in frame_list]\n",
    "\n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            if os.path.exists(image_path):\n",
    "                image = Image.open(image_path)\n",
    "                plt.subplot(num_rows, num_cols, i + 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(os.path.basename(image_path))\n",
    "                plt.axis('off')  \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def count_objects(self, objects):\n",
    "        object_count = {}\n",
    "        for obj in objects:\n",
    "            if obj in object_count:\n",
    "                object_count[obj] += 1\n",
    "            else:\n",
    "                object_count[obj] = 1\n",
    "        return object_count\n",
    "    \n",
    "    def get_object_from_frame(self, frame_name):\n",
    "        object_item_list = {}\n",
    "        for a_key, a_values in self.env.json_data['annotations'][frame_name].items():\n",
    "            for u_key, u_values in self.env.json_data['uidmap'].items():\n",
    "                if a_key in u_values:\n",
    "                    object_item_list[a_key] = u_key\n",
    "        return object_item_list\n",
    "    \n",
    "    def get_object(self, object_name):\n",
    "        object_list = {}\n",
    "        if object_name in self.env.json_data[\"uidmap\"]:\n",
    "            for item in self.env.json_data[\"uidmap\"][object_name]:\n",
    "                object_list[item] = object_name\n",
    "        else:\n",
    "            obj = self.issimilar(object_name, self.env.json_data[\"uidmap\"])\n",
    "            if obj != '':\n",
    "                object_list = self.get_object(obj)\n",
    "        return object_list\n",
    "    \n",
    "    def get_object_frames(self, object_list):\n",
    "        place_list = []\n",
    "        for key_to_find in object_list:\n",
    "            time_stamps = [time_stamp for time_stamp, keys in self.env.json_data['annotations'].items() if key_to_find in keys]\n",
    "            place_list = place_list + time_stamps\n",
    "\n",
    "        return place_list\n",
    "    \n",
    "    def shortest_path(self, start, goal):\n",
    "        graph = self.env.json_data['p-p']\n",
    "        n = len(graph)\n",
    "        distances = {node: float('inf') for node in range(n)}\n",
    "        distances[start] = 0\n",
    "        priority_queue = [(0, start)]\n",
    "        previous_nodes = {node: None for node in range(n)}\n",
    "\n",
    "        while priority_queue:\n",
    "            current_distance, current_node = heapq.heappop(priority_queue)\n",
    "\n",
    "            if current_node == goal:\n",
    "                path = []\n",
    "                while previous_nodes[current_node] is not None:\n",
    "                    path.append(current_node)\n",
    "                    current_node = previous_nodes[current_node]\n",
    "                path.append(start)\n",
    "                return path[::-1]\n",
    "\n",
    "            if current_distance > distances[current_node]:\n",
    "                continue\n",
    "\n",
    "            for neighbor, weight in enumerate(graph[current_node]):\n",
    "                if weight > 0:\n",
    "                    distance = current_distance + weight\n",
    "                    if distance < distances[neighbor]:\n",
    "                        distances[neighbor] = distance\n",
    "                        previous_nodes[neighbor] = current_node\n",
    "                        heapq.heappush(priority_queue, (distance, neighbor))\n",
    "\n",
    "        return None  \n",
    "    \n",
    "    def get_object_uids_list(self, frame_name):\n",
    "        object_item_list = {}\n",
    "        for a_key, a_values in self.env.json_data['annotations'][frame_name].items():\n",
    "            for u_key, u_values in self.env.json_data['uidmap'].items():\n",
    "                if a_key in u_values:\n",
    "                    object_item_list[a_key] = u_key\n",
    "        return object_item_list\n",
    "    \n",
    "    def issimilar(self, obj, object_list):\n",
    "        new_prompt = f'is {obj} in {object_list}'\n",
    "        messages = [{\"role\": \"system\", \"content\": \"user are asking if the given object things are in the object_list. Please just return the object name in object_list. if not, return \"\". For example: object = 'tv', object_list = {'bed': ['NB59gmIiC4u5h2Mw'], 'table': ['RnVg7UM3yU93OL1o', '53naDCpgHHmCVkxd'], 'cabinet': ['BOYx4gvUEXXzkHo0', 'FbEfcoVRieMmQ4IW'], 'tv_monitor': ['qJ0TKTnoAkhV0k0C']} This should return 'tv_monitor'. object = 'book', object_list = {'bed': ['NB59gmIiC4u5h2Mw'], 'table': ['RnVg7UM3yU93OL1o', '53naDCpgHHmCVkxd'], 'cabinet': ['BOYx4gvUEXXzkHo0', 'FbEfcoVRieMmQ4IW'], 'tv_monitor': ['qJ0TKTnoAkhV0k0C']} This should return ''  \"},\n",
    "                    {\"role\": \"user\", \"content\": new_prompt}]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def vlm(self, frame_number, text):\n",
    "        image_path = video_id + '_frames/lowres_wide/' + video_id + '_' + frame_number + '.png'\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            img = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "          model=\"gpt-4o-mini\",\n",
    "          messages=[\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\"type\": \"text\", \"text\": text},\n",
    "                {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{img}\",\n",
    "                  },\n",
    "                },\n",
    "              ],\n",
    "            }\n",
    "          ],\n",
    "          max_tokens=300,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7ffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tabletop_ui = '''\n",
    "#python EQA agent script\n",
    "#Generate the corresponding code according to the corresponding question\n",
    "\n",
    "#question: Show how many sampled_frames in this sample.\n",
    "#code:\n",
    "num = get_number_of_frames()\n",
    "say('f'There are {num} frames')\n",
    "\n",
    "#question: Show what kind of objects are there in frame number 3044.239 and the objects' quantities.\n",
    "#code:\n",
    "objects = get_object_from_frame(\"3044.239\")\n",
    "object_quantities = count_objects(objects)\n",
    "say(f'The objects in frame 3064.730 are: {object_quantities}')\n",
    "\n",
    "#question: Where is the table?\n",
    "#code:\n",
    "objects = get_object(\"table\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi I find there is a table in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: I am tired where can I go to sleep.\n",
    "#Hint: find bed!\n",
    "#code:\n",
    "objects = get_object(\"bed\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi I find there is a bed in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: Where can I put my bottle?\n",
    "#Hint: find table!\n",
    "#code:\n",
    "objects = get_object(\"table\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi you can put your bottle on the table: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: I am hungry where can I go to eat.\n",
    "#Hint: find table!\n",
    "#code:\n",
    "objects = get_object(\"table\")\n",
    "frames = get_object_frames(objects)\n",
    "say(f'Hi I find there is a table in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#question: \"I am in picture 3044.239 where can I go to 3133.735\"\n",
    "#code:\n",
    "start_node = get_frame2index(\"3044.239\")\n",
    "goal_node = get_frame2index(\"3133.735\")\n",
    "path = shortest_path(start_node, goal_node)\n",
    "frame_path = [get_index2frame(i) for i in path]\n",
    "say(f'Your path is {frame_path}')\n",
    "show_images_from_frames(frame_path)\n",
    "\n",
    "#question: \"are the tables in frame 3044.239 and frame 3105.730 the same?\"\n",
    "#code:\n",
    "objects_frame_1 = get_object_from_frame(\"3044.239\")\n",
    "objects_frame_2 = get_object_from_frame(\"3105.730\")\n",
    "tables_frame_1 = [key for key, values in objects_frame_1.items() if issimilar(values,[\"table\"])]\n",
    "tables_frame_2 = [key for key, values in objects_frame_2.items() if issimilar(values,[\"table\"])]\n",
    "\n",
    "if isequal(tables_frame_1,  tables_frame_2):\n",
    "    say(\"The tables in frame 3044.239 and frame 3105.730 are the same.\")\n",
    "else:\n",
    "    say(\"The tables in frame 3044.239 and frame 3105.730 are not the same.\")\n",
    "show_images_from_frames([\"3044.239\", \"3105.730\"])\n",
    "\n",
    "\n",
    "#question: does the table in frame number: 3044.239 appear in other frames?\n",
    "#code:\n",
    "objects = get_object_from_frame(\"3044.239\")\n",
    "target_objects = {key: values for key, values in objects.items() if issimilar(values,[\"table\"])}\n",
    "frames = get_object_frames(target_objects)\n",
    "say(f'Hi I find there is the same table in picture: {frames}')\n",
    "show_images_from_frames(frames)\n",
    "\n",
    "#important!!! if you get any question regarding color, shape, texture... anything you think scene-graph representations can't solve, than use the following vlm() funciton.\n",
    "#question: What's the color of the cabinet in picture 3123.722?\n",
    "#code:\n",
    "response = vlm(\"3123.722\", \"What's the color of the cabinet?\")\n",
    "say(f'{response}')\n",
    "show_images_from_frames([\"3123.722\"])\n",
    "\n",
    "#question: \n",
    "#code:\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe69bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fgen = '''\n",
    "#Generate the corresponding code according to the corresponding function define.\n",
    "\n",
    "#define function: ind = get_index(index)\n",
    "#code:\n",
    "def get_index(index):\n",
    "    return get_index2frame(index)\n",
    "    \n",
    "#define function: object = get_all_objects_in_space()\n",
    "#code:\n",
    "def get_all_objects_in_space():\n",
    "    object_list = []\n",
    "    env = get_env()\n",
    "    for item in env.json_data[\"uidmap\"].keys():\n",
    "        object_list.append(item)\n",
    "    return object_list\n",
    "    \n",
    "# define function: object_uids_list = get_object_from_frame(frame_name).\n",
    "#code:\n",
    "def get_object_from_frame(frame_name):\n",
    "    object_item_list = {}\n",
    "    env = get_env()\n",
    "    for a_key, a_values in env.json_data['annotations'][frame_name].items():\n",
    "        for u_key, u_values in env.json_data['uidmap'].items():\n",
    "            if a_key in u_values:\n",
    "                object_item_list[a_key] = u_key\n",
    "    return object_item_list\n",
    "\n",
    "#define function: result = isequal(a, b)\n",
    "#code:\n",
    "def isequal(a, b):\n",
    "    if a == b:\n",
    "        return True\n",
    "    return False\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f09218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_tabletop = {\n",
    "  'lmps': {\n",
    "    'tabletop_ui': {\n",
    "      'prompt_text': prompt_tabletop_ui,\n",
    "      'engine': model_name,\n",
    "      'max_tokens': 512,\n",
    "      'temperature': 0,\n",
    "      'query_prefix': '# ',\n",
    "      'query_suffix': '.',\n",
    "      'stop': ['#', 'objects = ['],\n",
    "      'maintain_session': True,\n",
    "      'debug_mode': False,\n",
    "      'include_context': True,\n",
    "      'has_return': False,\n",
    "      'return_val_name': 'ret_val',\n",
    "    },\n",
    "    'fgen': {\n",
    "      'prompt_text': prompt_fgen,\n",
    "      'engine': model_name,\n",
    "      'max_tokens': 512,\n",
    "      'temperature': 0,\n",
    "      'query_prefix': '# define function: ',\n",
    "      'query_suffix': '.',\n",
    "      'stop': ['# define', '# example'],\n",
    "      'maintain_session': False,\n",
    "      'debug_mode': False,\n",
    "      'include_context': True,\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aac3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_LMP(env, cfg_tabletop):\n",
    "    # LMP env wrapper\n",
    "    cfg_tabletop = copy.deepcopy(cfg_tabletop)\n",
    "    LMP_env = LMP_wrapper(env, cfg_tabletop)  \n",
    "    \n",
    "    # creating APIs that the LMPs can interact with\n",
    "    fixed_vars = {\n",
    "        'np': np, 'heapq': heapq\n",
    "    }\n",
    "    variable_vars = {\n",
    "      k: getattr(LMP_env, k)\n",
    "      for k in [\n",
    "         'get_env', 'get_index2frame', 'get_frame2index', 'show_images_from_frames',\n",
    "          'get_number_of_frames', 'count_objects', 'get_object_from_frame', 'get_object',\n",
    "          'get_object_frames', 'shortest_path', 'vlm', 'issimilar'\n",
    "      ]\n",
    "    }\n",
    "    variable_vars['say'] = lambda msg: print(f'robot says: {msg}')\n",
    "\n",
    "    # creating the function-generating LMP\n",
    "    lmp_fgen = LMPFGen(cfg_tabletop['lmps']['fgen'], fixed_vars, variable_vars)\n",
    "    \n",
    "    # creating other low-level LMPs\n",
    "    variable_vars.update({\n",
    "      k: LMP(k, cfg_tabletop['lmps'][k], lmp_fgen, fixed_vars, variable_vars)\n",
    "      for k in []\n",
    "    })\n",
    "    # creating the LMP that deals w/ high-level language commands\n",
    "    lmp_tabletop_ui = LMP(\n",
    "      'tabletop_ui', cfg_tabletop['lmps']['tabletop_ui'], lmp_fgen, fixed_vars, variable_vars\n",
    "    )\n",
    "\n",
    "    return lmp_tabletop_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1bc61d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'refine_topo_gt.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m MultiviewSceneGraph()\n\u001b[1;32m      2\u001b[0m lmp_tabletop_ui \u001b[38;5;241m=\u001b[39m setup_LMP(env, cfg_tabletop)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Example questions for the model to process\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#question = \"How many frames are there in the .\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#question = \"what kind of objects are there in the frame number 3044.722 and also give me the quantities\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#question = \"I’m in place 3044.239, how can I go to 3124.222?\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m##question = \"what do we have in this space\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mMultiviewSceneGraph.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#init\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefine_topo_gt.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/miniconda3/envs/msg/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'refine_topo_gt.json'"
     ]
    }
   ],
   "source": [
    "env = MultiviewSceneGraph()\n",
    "lmp_tabletop_ui = setup_LMP(env, cfg_tabletop)\n",
    "\n",
    "#Example questions for the model to process\n",
    "#question = \"How many frames are there in the .\"\n",
    "#question = \"what kind of objects are there in the frame number 3044.722 and also give me the quantities\"\n",
    "#question = \"show me where are the tables.\"\n",
    "#question = \"Where can I put my laptop.\"\n",
    "#question = \"I am tired where can I go to sleep\"\n",
    "#question = \"I’m in place 3044.239, how can I go to 3124.222?\"\n",
    "##question = \"what do we have in this space\"\n",
    "\n",
    "question = \"are the tables in frame 3044.239 and frame 3105.730 the same?\"\n",
    "#question = \"are the tables in frame 3044.239 and frame 3044.722 the same?\"\n",
    "#question = \"does the table in frame number: 3044.239 appear in other frames?\"\n",
    "#question = \"does the tv in frame number: 3127.721 appear in other frames?\"\n",
    "#question = \"is there any book in this space?\"\n",
    "#question = \"How many tv are there.\"\n",
    "#question = \"What's the color of the cabinet in picture 3123.722?\"\n",
    "#question = \"What's the shape of the table in picture 3044.239?\"\n",
    "\n",
    "user_input = question #@param {allow-input: true, type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1facc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lmp_tabletop_ui' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lmp_tabletop_ui(user_input, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lmp_tabletop_ui' is not defined"
     ]
    }
   ],
   "source": [
    "lmp_tabletop_ui(user_input, f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57415ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03fee84e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2899127317.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [45]\u001b[1;36m\u001b[0m\n\u001b[1;33m    1. \"Add object comparison for specific video frames\"\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "1. \"Added object comparison for specific video frames\"\n",
    "2. \"Added command to check whether object appearance in specific frames\"\n",
    "3. \"Added `issimilar` function for fuzzy object name matching\"\n",
    "4. \"Added VLM access.\"\n",
    "5. \"Added localization function.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
